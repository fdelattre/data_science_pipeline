{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisation de FastText pour la classification supervisée de texte\n",
    "\n",
    "- Introduction\n",
    "- Installation de Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up dataset...\n",
      "created train.txt and test.txt...\n",
      "('Total running time: ', 4.430346)\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "\n",
    "import sys\n",
    "import codecs\n",
    "import smart_open\n",
    "import random\n",
    "from string import digits, punctuation\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "if sys.version > '3':\n",
    "    control_chars = [chr(0x85)]\n",
    "else:\n",
    "    control_chars = [unichr(0x85)]\n",
    "\n",
    "dirname = 'txt_sentoken'\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stopword_set = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(d):\n",
    "    new_d = re.sub(r'[0-9]+', '', d)\n",
    "    dlist = tokenizer.tokenize(new_d.lower())\n",
    "    wlist = [token for token in dlist if token not in stopword_set]\n",
    "    return \" \".join(wlist)\n",
    "\n",
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "    norm_text = norm_text.replace('</s>', '')\n",
    "    norm_text = norm_text.replace('new york', 'newyork')\n",
    "    # Remove punctuation\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':', '*', '-', '\\'', '[', ']', '`', '/', '<', '>']:\n",
    "        norm_text = norm_text.replace(char, ' ')\n",
    "    return norm_text\n",
    "\n",
    "import time\n",
    "start = time.clock()\n",
    "\n",
    "# Concatenate and normalize test/train data\n",
    "print(\"Cleaning up dataset...\")\n",
    "folders = ['pos', 'neg']\n",
    "alldata = u''\n",
    "for fol in folders:\n",
    "    temp = u''\n",
    "    # Is there a better pattern to use?\n",
    "    txt_files = glob.glob(os.path.join(dirname, fol, '*.txt'))\n",
    "    for txt in txt_files:\n",
    "        with smart_open.smart_open(txt, \"rb\") as t:\n",
    "            t_clean = t.read().decode(\"ascii\")\n",
    "            t_clean = t_clean.replace('\\n', '')\n",
    "            t_clean = clean_text(t_clean)\n",
    "            for c in control_chars:\n",
    "                t_clean = t_clean.replace(c, ' ')\n",
    "                \n",
    "            temp += '__label__{0} '.format(fol) + clean_text(t_clean)\n",
    "        temp += \"\\n\"\n",
    "    temp_norm = normalize_text(temp)\n",
    "    alldata += temp_norm\n",
    "    \n",
    "all_data_list = alldata.splitlines()\n",
    "random.shuffle(all_data_list)\n",
    "train = all_data_list[0:1600]\n",
    "test  = all_data_list[1601:]\n",
    "        \n",
    "with smart_open.smart_open(os.path.join(dirname, 'train.txt'), 'wb') as f:\n",
    "    for idx, line in enumerate(train):\n",
    "        num_line = u\"{0}\\n\".format(line)\n",
    "        f.write(num_line.encode(\"utf-8\"))\n",
    "        \n",
    "with smart_open.smart_open(os.path.join(dirname, 'test.txt'), 'wb') as f:\n",
    "    for idx, line in enumerate(test):\n",
    "        num_line = u\"{0}\\n\".format(line)\n",
    "        f.write(num_line.encode(\"utf-8\"))\n",
    "\n",
    "print(\"created train.txt and test.txt...\")\n",
    "\n",
    "end = time.clock()\n",
    "print (\"Total running time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  7891\n",
      "Number of labels: 2\n",
      "Progress: 100.0%  words/sec/thread: 1197326  lr: 0.000000  loss: 0.058987  eta: 0h0m \n"
     ]
    }
   ],
   "source": [
    "! /home/francois/Projets/fasttext/fastText-0.1.0/fasttext supervised -input txt_sentoken/train.txt -output ft_model -dim 100 -minCount 10 -lr 1.0 -epoch 50 -wordNgrams 2 -verbose 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t399\r\n",
      "P@1\t0.86\r\n",
      "R@1\t0.86\r\n",
      "Number of examples: 399\r\n"
     ]
    }
   ],
   "source": [
    "! /home/francois/Projets/fasttext/fastText-0.1.0/fasttext test ft_model.bin txt_sentoken/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! /home/francois/Projets/fasttext/fastText-0.1.0/fasttext predict ft_model.bin txt_sentoken/test.txt > txt_sentoken/predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[169  30]\n",
      " [ 26 174]]\n",
      "0.852941176471\n",
      "0.87\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "predictions = pd.read_table('txt_sentoken/predictions.txt', header=None, names=['sent'])\n",
    "\n",
    "true_label = []\n",
    "for idx, line in enumerate(test):\n",
    "    true_label.append(line.split(' ')[0])\n",
    "\n",
    "true_label = pd.Series(true_label)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "\n",
    "print(confusion_matrix(y_true=true_label, y_pred=predictions.sent))\n",
    "print(precision_score(y_true=true_label, y_pred=predictions.sent, pos_label=u'__label__pos'))\n",
    "print(recall_score(y_true=true_label, y_pred=predictions.sent, pos_label=u'__label__pos'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des coordonnées dans l'espace de représentation pour une phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0089917 -0.0648 0.011288 -0.048513 -0.042958 -0.020031 0.024691 -0.043624 0.07417 0.060329 -0.058447 0.011822 -0.033099 -0.069808 -0.041454 0.0083163 -0.073101 0.0016414 0.0033266 0.025806 0.0020999 0.069131 -0.03313 -0.0055453 -0.046971 0.041006 -0.017689 0.03461 -0.016446 -0.02283 -0.014837 0.032038 -0.048377 0.025368 -0.0080721 0.043123 -0.035591 0.027284 0.055823 0.056293 -0.05315 -0.022388 0.020224 0.0027797 -0.013748 0.040548 0.028991 -0.015892 0.017652 0.02129 -0.047755 -0.03256 -0.0099254 -0.043222 0.014896 0.010894 0.063251 0.0041321 0.0056546 0.006915 0.074451 0.021588 -0.035139 0.01356 0.025366 -0.012054 -0.05446 -0.084969 0.030065 -0.016906 0.08094 0.067511 0.022843 -0.0057636 -0.052231 -0.00010444 0.021866 -0.03161 0.0028517 -0.030354 0.047949 -0.090013 -0.067344 -0.045949 0.019621 -0.04837 -0.079511 0.067844 0.0099889 -0.013375 -0.0051241 0.056258 0.010435 0.011498 -0.027954 0.082473 -0.065607 -0.013554 0.042453 -0.025248 \r\n"
     ]
    }
   ],
   "source": [
    "! echo \"this is movie a nice\" | /home/francois/Projets/fasttext/fastText-0.1.0/fasttext print-sentence-vectors ft_model.bin "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction sur une phrase du jeu de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__neg staring george clooney arnold schwarzenegger chris donnell uma thurman alicia silverstone well start like say couple things first miss michael keaton miss tim burton would much prefer think last two batman films like dallas dream sequence even first film batman really star though damn close first couple anyway batman forever batman robin almost pushed bit player really say clooney regardless better kilmer good batman given next nothing might well made much difference joel schumacher said refuses bend masses hated films cheered return buton wont make batman brooding dark fine mean granted batman returns awsome film bit dark sometimes yet bright neon campy style killing anything series meant usually easy films loved con air films critics slammed granted film really paper thin cliche ridden except one thing fun virtually fun film cared less thrill little ones theres violence really none everyone comes fine end like old tv shows ends everyone laughing schwarzenegger awful mean really really bad coming regular fan work love movies time basic reason never says much talks often movie would much prefered seen patric stewart roll believe truely like film care least someone film liked alfred reason behind cheap shot\n",
      "__label__neg 0.992187\n"
     ]
    }
   ],
   "source": [
    "! shuf -n 1 txt_sentoken/test.txt > demo_sentence\n",
    "! cat demo_sentence\n",
    "! cat demo_sentence| /home/francois/Projets/fasttext/fastText-0.1.0/fasttext predict-prob ft_model.bin -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Prédiction sur une nouvelle phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__neg\r\n"
     ]
    }
   ],
   "source": [
    "! echo \"this is worst film ever\" | /home/francois/Projets/fasttext/fastText-0.1.0/fasttext predict ft_model.bin -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__pos\r\n"
     ]
    }
   ],
   "source": [
    "! echo \"this is the best film ever\" | /home/francois/Projets/fasttext/fastText-0.1.0/fasttext predict ft_model.bin -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédiction avec proba sur une nouvelle phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__pos 0.996094\r\n"
     ]
    }
   ],
   "source": [
    "! echo \"want like mike mike badly embarrassingly bad broke six year relationship six months ago move n l still result jokes fall flat tries impress comedian well unemployed comedian one hollywood little fish gotten bad asked application starbuck actually starbuck thing gets even worse worse fail like mike however played endearingly jon favreau swingers revolves around mike half hearted awkward efforts get back social swing things end enthusiastically assisted equally underachieving twentysomething actor friends trent vince vaughn film animated performance gift gab bordering disturbing side motivational speaking gun toting sue patrick van horn rob ron livingston played hamlet longs work goofy disneyland succeed lack theme park experience together talk women make eye contact involves avoiding call never appear desperate two days industry standard talking talk walk walk strutting like cool swingers aspire always laughable effect get mike funk friends persistently convince get outside whether quickie night trip seedy vegas casino cutthroat hollywood party beautiful people 50s swing lounge everywhere encourage mike look replacement honey keep vigil progress lack like dotty doting parents mike never leaves company without recipient confidence cheerleading money mike honeys know equipped eccentric dating philosophies analogy flirting bear bunny unwavering support friends like maybe mike badly favreau also doubled film screenwriter proves gift creating engaging characters witty banter goes beyond today bon mot reassuring honesty friendships even hits low points friends commiserate mike gets gumption look love join cheerleading section well swingers light unassuming fare sweet candy want bust gut laughing\" | /home/francois/Projets/fasttext/fastText-0.1.0/fasttext predict-prob ft_model.bin -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003392 -0.002939 0.0019247 -0.0028346 -0.0047066 -0.00016678 0.0071454 -0.0054693 0.0041421 0.0032568 -0.0038942 0.00316 0.0020336 -0.0031915 -0.0021873 -0.0012865 -0.0073408 0.0026556 0.0015705 0.0028794 0.0011919 0.0076576 -0.0027339 -0.00035268 -0.0036188 0.0069926 -0.0024973 0.0031581 -0.0034432 0.00045934 -0.0047447 -0.0018983 -0.0072065 -0.00043172 -0.0013393 0.0064714 -0.0028161 0.00093948 0.0024238 0.0060359 -0.00098259 0.0038408 0.0033328 0.0015892 -0.0010333 -3.2415e-05 0.0038734 -0.001281 -0.0013905 0.0051946 -0.0034965 0.00033689 0.00027116 -0.00072945 0.0033516 0.00067717 0.0035627 0.0027791 0.0057138 0.0019676 0.0024891 0.0041746 -0.0024664 -0.00050395 0.00019104 -0.00336 -0.0075722 -0.0066707 0.0011618 0.0057644 0.0022545 0.0042548 0.00086093 0.0015778 -0.0067326 0.0012711 0.001366 -0.0046021 -0.00069335 -0.0042968 0.0041013 -0.00421 -0.0028406 -0.0063894 0.0029337 -0.0062338 -0.0038646 0.0081928 0.005114 -0.00094606 -0.0022145 0.0072953 0.0028441 -0.0002434 -0.0029185 0.0046113 -0.0049927 -0.00069436 0.0020726 -0.0069913 \r\n"
     ]
    }
   ],
   "source": [
    "! echo \"good film\" | /home/francois/Projets/fasttext/fastText-0.1.0/fasttext print-sentence-vectors ft_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003392 -0.002939 0.0019247 -0.0028346 -0.0047066 -0.00016678 0.0071454 -0.0054693 0.0041421 0.0032568 -0.0038942 0.00316 0.0020336 -0.0031915 -0.0021873 -0.0012865 -0.0073408 0.0026556 0.0015705 0.0028794 0.0011919 0.0076576 -0.0027339 -0.00035268 -0.0036188 0.0069926 -0.0024973 0.0031581 -0.0034432 0.00045934 -0.0047447 -0.0018983 -0.0072065 -0.00043172 -0.0013393 0.0064714 -0.0028161 0.00093948 0.0024238 0.0060359 -0.00098259 0.0038408 0.0033328 0.0015892 -0.0010333 -3.2415e-05 0.0038734 -0.001281 -0.0013905 0.0051946 -0.0034965 0.00033689 0.00027116 -0.00072945 0.0033516 0.00067717 0.0035627 0.0027791 0.0057138 0.0019676 0.0024891 0.0041746 -0.0024664 -0.00050395 0.00019104 -0.00336 -0.0075722 -0.0066707 0.0011618 0.0057644 0.0022545 0.0042548 0.00086093 0.0015778 -0.0067326 0.0012711 0.001366 -0.0046021 -0.00069335 -0.0042968 0.0041013 -0.00421 -0.0028406 -0.0063894 0.0029337 -0.0062338 -0.0038646 0.0081928 0.005114 -0.00094606 -0.0022145 0.0072953 0.0028441 -0.0002434 -0.0029185 0.0046113 -0.0049927 -0.00069436 0.0020726 -0.0069913 \r\n"
     ]
    }
   ],
   "source": [
    "! echo \"__label__neg good film\" | /home/francois/Projets/fasttext/fastText-0.1.0/fasttext print-sentence-vectors ft_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! cat txt_sentoken/test.txt | /home/francois/Projets/fasttext/fastText-0.1.0/fasttext print-sentence-vectors ft_model.bin > test_embeddings.txt\n",
    "! cat txt_sentoken/train.txt | /home/francois/Projets/fasttext/fastText-0.1.0/fasttext print-sentence-vectors ft_model.bin > train_embeddings.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = pd.read_table('train_embeddings.txt', header=None, sep = ' ', usecols=range(100))\n",
    "test_embeddings  = pd.read_table('test_embeddings.txt', header=None, sep = ' ', usecols=range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001063</td>\n",
       "      <td>0.012428</td>\n",
       "      <td>-0.002059</td>\n",
       "      <td>0.008208</td>\n",
       "      <td>0.007955</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>-0.004256</td>\n",
       "      <td>0.007651</td>\n",
       "      <td>-0.012737</td>\n",
       "      <td>-0.011058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>-0.010139</td>\n",
       "      <td>-0.001360</td>\n",
       "      <td>-0.002075</td>\n",
       "      <td>0.005754</td>\n",
       "      <td>-0.014911</td>\n",
       "      <td>0.011132</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>-0.007941</td>\n",
       "      <td>0.004074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001401</td>\n",
       "      <td>0.013775</td>\n",
       "      <td>-0.002551</td>\n",
       "      <td>0.009404</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.004084</td>\n",
       "      <td>-0.004653</td>\n",
       "      <td>0.008620</td>\n",
       "      <td>-0.014729</td>\n",
       "      <td>-0.012456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>-0.011186</td>\n",
       "      <td>-0.002151</td>\n",
       "      <td>-0.001905</td>\n",
       "      <td>0.005954</td>\n",
       "      <td>-0.016754</td>\n",
       "      <td>0.012882</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>-0.009022</td>\n",
       "      <td>0.004427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001421</td>\n",
       "      <td>0.013207</td>\n",
       "      <td>-0.001691</td>\n",
       "      <td>0.008903</td>\n",
       "      <td>0.008507</td>\n",
       "      <td>0.003653</td>\n",
       "      <td>-0.004594</td>\n",
       "      <td>0.007651</td>\n",
       "      <td>-0.013300</td>\n",
       "      <td>-0.011135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>-0.010526</td>\n",
       "      <td>-0.001708</td>\n",
       "      <td>-0.001801</td>\n",
       "      <td>0.005987</td>\n",
       "      <td>-0.015382</td>\n",
       "      <td>0.012073</td>\n",
       "      <td>0.002372</td>\n",
       "      <td>-0.007910</td>\n",
       "      <td>0.004628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000999</td>\n",
       "      <td>-0.012914</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>-0.009833</td>\n",
       "      <td>-0.008372</td>\n",
       "      <td>-0.003287</td>\n",
       "      <td>0.004825</td>\n",
       "      <td>-0.007997</td>\n",
       "      <td>0.014222</td>\n",
       "      <td>0.012257</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000974</td>\n",
       "      <td>0.010504</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>-0.005833</td>\n",
       "      <td>0.015750</td>\n",
       "      <td>-0.012353</td>\n",
       "      <td>-0.002224</td>\n",
       "      <td>0.008334</td>\n",
       "      <td>-0.004855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.001388</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>-0.002300</td>\n",
       "      <td>0.009498</td>\n",
       "      <td>0.008577</td>\n",
       "      <td>0.003838</td>\n",
       "      <td>-0.004225</td>\n",
       "      <td>0.008730</td>\n",
       "      <td>-0.014297</td>\n",
       "      <td>-0.012252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>-0.011179</td>\n",
       "      <td>-0.001871</td>\n",
       "      <td>-0.002441</td>\n",
       "      <td>0.005960</td>\n",
       "      <td>-0.015975</td>\n",
       "      <td>0.012650</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>-0.009122</td>\n",
       "      <td>0.004612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.001063  0.012428 -0.002059  0.008208  0.007955  0.003319 -0.004256   \n",
       "1 -0.001401  0.013775 -0.002551  0.009404  0.008693  0.004084 -0.004653   \n",
       "2 -0.001421  0.013207 -0.001691  0.008903  0.008507  0.003653 -0.004594   \n",
       "3  0.000999 -0.012914  0.001757 -0.009833 -0.008372 -0.003287  0.004825   \n",
       "4 -0.001388  0.013559 -0.002300  0.009498  0.008577  0.003838 -0.004225   \n",
       "\n",
       "         7         8         9     ...           90        91        92  \\\n",
       "0  0.007651 -0.012737 -0.011058    ...     0.000523 -0.010139 -0.001360   \n",
       "1  0.008620 -0.014729 -0.012456    ...     0.001362 -0.011186 -0.002151   \n",
       "2  0.007651 -0.013300 -0.011135    ...     0.000633 -0.010526 -0.001708   \n",
       "3 -0.007997  0.014222  0.012257    ...    -0.000974  0.010504  0.002103   \n",
       "4  0.008730 -0.014297 -0.012252    ...     0.000934 -0.011179 -0.001871   \n",
       "\n",
       "         93        94        95        96        97        98        99  \n",
       "0 -0.002075  0.005754 -0.014911  0.011132  0.001784 -0.007941  0.004074  \n",
       "1 -0.001905  0.005954 -0.016754  0.012882  0.002448 -0.009022  0.004427  \n",
       "2 -0.001801  0.005987 -0.015382  0.012073  0.002372 -0.007910  0.004628  \n",
       "3  0.002106 -0.005833  0.015750 -0.012353 -0.002224  0.008334 -0.004855  \n",
       "4 -0.002441  0.005960 -0.015975  0.012650  0.002226 -0.009122  0.004612  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "for idx, line in enumerate(train):\n",
    "    if line.split(' ')[0] == '__label__neg':\n",
    "        train_labels.append(0)\n",
    "    else:\n",
    "        train_labels.append(1)\n",
    "    \n",
    "train_labels = pd.Series(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisation en mode non supervisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cut -d \" \" -f2- < txt_sentoken/train.txt > txt_sentoken/new_train.txt\n",
    "! cut -d \" \" -f2- < txt_sentoken/test.txt > txt_sentoken/new_test.txt\n",
    "! cat txt_sentoken/new_train.txt txt_sentoken/new_test.txt > txt_sentoken/alltext.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  14445\n",
      "Number of labels: 0\n",
      "Progress: 100.0%  words/sec/thread: 40345  lr: 0.000000  loss: 2.307617  eta: 0h0m \n"
     ]
    }
   ],
   "source": [
    "! /home/francois/Projets/fasttext/fastText-0.1.0/fasttext skipgram -input txt_sentoken/alltext.txt -output ft_unsup -verbose 1 -epoch 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie 0.14064 0.22668 -0.21277 0.11936 0.12501 0.10321 -0.2514 -0.29288 -0.29551 -0.35803 0.31854 0.055502 -0.21604 -0.15882 0.16071 0.047071 0.34528 0.28839 0.18431 0.039957 -0.049985 0.17658 0.053777 -0.098213 -0.060322 0.24786 0.1271 0.44754 -0.088739 0.12483 -0.19556 0.039624 0.04636 0.030464 -0.16416 0.054768 -0.1555 -0.21387 -0.05512 -0.2886 -0.1767 -0.03436 -0.037316 -0.081621 -0.086023 0.048882 0.19246 0.050984 0.18906 -0.067273 -0.285 -0.076275 -0.016961 -0.17284 0.1082 0.22225 0.20788 0.072995 0.094063 -0.32515 -0.048841 -0.18102 0.10388 0.20685 -0.058732 0.2537 0.13877 0.035383 -0.33446 0.14121 0.055363 -0.075047 -0.01501 0.2677 -0.17807 -0.22202 -0.023567 -0.20686 0.086915 -0.22158 -0.015467 0.21271 0.30773 0.16065 0.029145 0.040464 0.29892 0.2771 -0.02625 0.047435 0.0054103 -0.037923 -0.0018578 0.30257 0.2315 0.048956 0.078198 -0.066184 0.14997 -0.021119 \r\n"
     ]
    }
   ],
   "source": [
    "! echo \"movie\" | /home/francois/Projets/fasttext/fastText-0.1.0/fasttext print-word-vectors ft_unsup.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing word vectors... done.\n",
      "Query word? kjv 0.4999\n",
      "gods 0.499604\n",
      "gosh 0.487426\n",
      "rayden 0.481517\n",
      "ohh 0.476493\n",
      "prayer 0.468075\n",
      "sebastian 0.463756\n",
      "prophecy 0.462057\n",
      "godfather 0.462025\n",
      "forbid 0.459193\n",
      "Query word? "
     ]
    }
   ],
   "source": [
    "! echo 'god' | /home/francois/Projets/fasttext/fastText-0.1.0/fasttext nn ft_unsup.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really loved this film -0.025931 -0.073626 -0.010985 0.020291 -0.043761 -0.036222 -0.0042842 0.074306 0.021662 0.060196 0.078136 -0.18185 0.0040928 -0.082199 -0.034199 -0.01356 0.031131 -0.01258 0.041837 0.18099 0.036069 -0.093139 -0.026138 0.000578 0.04385 -0.071343 -0.061684 0.086262 -0.01204 -0.054685 -0.079823 0.062614 -0.028148 0.034306 0.12456 0.024237 -0.042043 -0.087541 -0.0067077 0.070211 0.034819 0.050928 0.012006 -0.0030801 0.075727 0.028465 0.063046 0.010449 0.030095 0.038868 -0.10905 0.10011 -0.071911 0.10821 0.13392 -0.0045151 -0.027522 -0.035445 -0.036866 -0.078188 0.030399 -0.026785 0.072355 0.088656 -0.0015046 -0.099272 -0.10924 0.027447 -0.027323 -0.020139 0.029448 -0.028611 -0.01919 -0.12224 -0.025228 -0.018938 -0.058468 -0.1063 0.060376 0.052353 0.0093961 0.080436 0.05728 0.061197 0.10581 -0.027898 -0.037431 0.01361 0.029897 0.079944 0.098863 -0.063421 0.078235 -0.075797 -0.12327 -0.0076739 -0.065259 0.032764 0.026322 -0.060646 \r\n"
     ]
    }
   ],
   "source": [
    "! echo 'I really loved this film' | /home/francois/Projets/fasttext/fastText-0.1.0/fasttext print-sentence-vectors ft_unsup.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisation d'un modèle pré-entrainé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nous aimons la bière 0.0097061 0.027632 -0.016972 0.0024993 -0.059054 -0.049652 -0.038861 0.067473 -0.027538 0.04983 -0.101 0.051885 -0.060752 0.043942 0.00046987 -0.065984 0.032154 0.013981 -0.0010329 -0.027698 0.046101 -0.067757 -0.032105 0.048581 0.018488 -0.054423 -0.028525 -0.03788 -0.013619 -0.036317 0.052851 0.01124 0.0069892 0.03149 0.04434 -0.028495 -0.026408 -0.037046 0.040003 -0.025441 0.031822 -0.033723 0.037992 -0.0064203 0.050544 -0.01959 0.017874 0.058403 -0.01319 0.024226 0.012506 0.004137 -0.022506 0.072635 -0.02259 -0.050828 -0.073293 -0.03184 0.058128 -0.01114 -0.023823 0.083778 0.073318 0.036844 -0.036143 0.027283 -0.0095662 -0.0055503 0.031119 -0.069717 0.014842 0.019191 -0.098134 0.0060344 -0.092135 -0.0078774 0.071788 0.069902 -0.017464 0.02453 -0.038238 -0.015698 -0.028161 -0.036732 0.0016258 0.055754 0.047618 0.028653 -0.005505 -0.091129 -0.017169 0.0024231 -0.021791 -0.050251 -0.012768 0.015336 -0.046974 0.013884 0.011046 -0.081225 0.058553 -0.025834 -0.0062332 0.067528 0.010848 0.019963 0.023921 -0.046774 0.0040594 0.033235 -0.0078621 0.038327 -0.035953 0.055456 0.015334 -0.04307 0.027753 0.024413 0.0084431 0.028719 0.0094911 -0.06293 -0.034197 0.037991 0.042708 -0.0027479 -0.0017034 0.039776 0.028631 -0.015151 -0.0045296 -0.0052217 0.074073 0.021545 -0.017001 0.040032 0.058833 0.015529 0.022464 0.021246 -0.003855 0.043202 -0.031907 -0.023504 -0.0088054 -0.032874 -0.059108 0.023855 -0.026056 -0.022824 -0.016577 -0.020769 0.0039216 -0.0086018 0.038684 -0.015129 -0.0036365 0.020799 -0.0078357 0.067579 -0.026637 -0.034311 0.022867 -0.03046 -0.0052465 0.036457 0.0052955 -0.016039 0.024526 -0.0077553 0.020812 -0.039709 -0.044565 0.048834 0.031678 -0.023191 0.022473 -0.014018 0.00077316 -0.041176 0.0015309 -0.051248 0.024707 0.043027 0.0092763 0.00051507 -0.027925 -0.047089 -0.023316 0.0054541 -0.037746 -0.0035625 -0.00091109 -0.0037935 -0.011691 -0.015177 -0.0044496 -0.01986 -0.013469 0.056333 0.015709 -0.017791 0.016622 0.019742 -0.075718 -0.02127 -0.016713 -0.047287 0.0089562 0.022252 0.0084778 -0.0086033 0.016931 -0.012427 -0.082249 0.045308 0.019243 -0.015439 0.042928 0.073537 -0.016287 4.2406e-05 -0.034442 -0.024152 0.012702 -0.028508 -0.035934 -0.062492 -0.0032299 0.0085668 -0.01015 0.0040516 -0.029312 -0.029044 -0.009899 -0.026469 0.028149 -0.010759 0.050149 -0.00030135 -0.046149 0.016447 0.0035925 0.017868 0.019998 -0.0076525 -0.020916 -0.023897 0.022402 0.019444 -0.020444 0.066801 0.0044122 0.018293 0.013445 -0.056142 0.019407 -0.04068 0.03792 0.04541 -0.017754 -0.042889 0.0097648 0.018409 0.0096456 -0.071154 0.0038808 -0.0049103 -0.013933 -0.011747 0.016071 -0.020925 0.03672 -0.016959 -0.031045 -0.026315 -0.025727 -0.081597 -0.056647 -0.039235 -0.012388 0.0023367 0.0042459 0.031919 -0.027094 0.042214 0.027112 0.097896 0.0027587 -0.0017227 -0.027375 -0.016229 0.042362 -0.0085321 0.0098643 -0.044688 -0.047985 0.045052 0.012944 -0.0074367 \r\n"
     ]
    }
   ],
   "source": [
    "! echo \"Nous aimons la bière\" | /home/francois/Projets/fasttext/fastText-0.1.0/fasttext print-sentence-vectors /home/francois/Projets/fasttext/wiki.fr.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing word vectors... done.\n",
      "Query word? dieu  0.712345\n",
      "divinité 0.702067\n",
      "dieu, 0.696018\n",
      "dieux 0.689006\n",
      "divin 0.689001\n",
      "divine 0.659328\n",
      "divine» 0.657204\n",
      "dieu» 0.650595\n",
      "dieux» 0.64837\n",
      "démiurge 0.643392\n",
      "Query word? "
     ]
    }
   ],
   "source": [
    "! echo 'dieu' | /home/francois/Projets/fasttext/fastText-0.1.0/fasttext nn /home/francois/Projets/fasttext/wiki.fr.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
